{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "from chatgpt_util_imports import generate_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load +31k patches as compiled by `bwhitman` for `learnfm`.\n",
    "fpath = 'data/compact.bin'\n",
    "\n",
    "df = generate_dataset(fpath)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "print(f\"Num patches: {len(df)}\")\n",
    "print(f\"Num features: {len(df.keys())}\")\n",
    "\n",
    "# df.hist(figsize=(50, 50))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        if 'VOICE NAME' in df.keys():\n",
    "            df = df.drop(columns=['VOICE NAME'])\n",
    "        \n",
    "        self.parameter_names = df.keys()\n",
    "        self.means = df.mean()\n",
    "        self.stds = df.std()\n",
    "\n",
    "        df = df - df.mean()\n",
    "        df = df / df.std()\n",
    "\n",
    "        self.df = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatchDataset(df)\n",
    "\n",
    "p_train = 0.8\n",
    "batch_size = 32\n",
    "\n",
    "n_train = int(p_train * len(dataset))\n",
    "n_val = len(dataset) - n_train\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\" \n",
    "    Variational autoencoder class, as per _Auto-Encoding Variational Bayes_. \n",
    "        https://arxiv.org/pdf/1312.6114.pdf\n",
    "\n",
    "    Code written with reference to:\n",
    "        https://github.com/AntixK/PyTorch-VAE/\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, n_hidden, n_latent):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_latent),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_latent),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_latent, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_features),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(n_latent, n_latent)\n",
    "        self.fc_var = nn.Linear(n_latent, n_latent)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # From latent embedding, generate mean and standard deviation.\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "\n",
    "        return [mu, log_var]\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        # Render as a random sample from Gaussian distribution.\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "\n",
    "        # Latent representation.\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        y = self.decode(z)\n",
    "        return [y, mu, log_var]\n",
    "\n",
    "def vae_loss(x, y, mu, log_var, kld_weight=1.0):\n",
    "\n",
    "    reconstruction_loss = F.mse_loss(x, y)\n",
    "    kullback_liebler_loss = torch.mean(\n",
    "        -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1), dim=0\n",
    "    )\n",
    "\n",
    "    loss = reconstruction_loss + kld_weight * kullback_liebler_loss\n",
    "    return {'loss': loss, 'mse': reconstruction_loss, 'kld': kullback_liebler_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(dataset.parameter_names)\n",
    "n_hidden = 64\n",
    "n_latent = 16\n",
    "\n",
    "model = VAE(n_features, n_hidden, n_latent)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.03139978, (MSE: 0.03139829, KLD: 0.00000150)\n",
      "Epoch 0, Val loss: 0.03211069, (MSE: 0.03210773, KLD: 0.00000296)\n",
      "Epoch 1, Train loss: 0.03139569, (MSE: 0.03139399, KLD: 0.00000170)\n",
      "Epoch 1, Val loss: 0.03206557, (MSE: 0.03206410, KLD: 0.00000146)\n",
      "Epoch 2, Train loss: 0.03140579, (MSE: 0.03140438, KLD: 0.00000141)\n",
      "Epoch 2, Val loss: 0.03207097, (MSE: 0.03206869, KLD: 0.00000228)\n",
      "Epoch 3, Train loss: 0.03140994, (MSE: 0.03140814, KLD: 0.00000181)\n",
      "Epoch 3, Val loss: 0.03205190, (MSE: 0.03204911, KLD: 0.00000279)\n"
     ]
    }
   ],
   "source": [
    "# Training.\n",
    "n_epochs = 10\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_train_loss = {key: 0.0 for key in ['loss', 'mse', 'kld']}\n",
    "    epoch_val_loss = {key: 0.0 for key in ['loss', 'mse', 'kld']}\n",
    "\n",
    "    for x in train_loader:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y, mu, log_var = model(x)\n",
    "        loss = vae_loss(x, y, mu, log_var)\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for key in loss.keys():\n",
    "            epoch_train_loss[key] += loss[key].item()\n",
    "\n",
    "    # Report average loss.\n",
    "    for key in loss.keys():\n",
    "        epoch_train_loss[key] /= len(train_dataset)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in val_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            y, mu, log_var = model(x)\n",
    "            loss = vae_loss(x, y, mu, log_var)\n",
    "\n",
    "            for key in loss.keys():\n",
    "                epoch_val_loss[key] += loss[key].item()\n",
    "\n",
    "        for key in loss.keys():\n",
    "                epoch_val_loss[key] /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {i}, Train loss: {epoch_train_loss['loss']:.8f}, (MSE: {epoch_train_loss['mse']:.8f}, KLD: {epoch_train_loss['kld']:.8f})\")\n",
    "    print(f\"Epoch {i}, Val loss: {epoch_val_loss['loss']:.8f}, (MSE: {epoch_val_loss['mse']:.8f}, KLD: {epoch_val_loss['kld']:.8f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
